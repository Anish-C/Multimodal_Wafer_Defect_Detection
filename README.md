# Multimodal_Wafer_Defect_Detection
# Multi-Modal Deep Learning for Semiconductor Wafer Defect Detection

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org/)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/your-notebook-id)

## üéØ Project Overview

This project implements a **multi-modal deep learning framework** that integrates wafer map image data with high-dimensional process sensor measurements to improve semiconductor defect detection and classification. Our approach combines CNNs for image processing and MLPs for sensor data analysis, unified through attention-based fusion mechanisms.

### üèÜ Key Achievements
- **Multi-modal Architecture**: Novel fusion of image and sensor data modalities
- **Attention Mechanism**: Dynamic weighting of modality contributions
- **Physics-Informed Design**: Sensor grouping based on manufacturing domain knowledge
- **Class Imbalance Handling**: Balanced sampling and weighted loss functions

## üìä Results Summary

| Metric | Value | Notes |
|--------|-------|-------|
| **Overall Accuracy** | 34.01% | [31.71%, 36.32%] 95% CI |
| **Macro F1-Score** | 0.3184 | Balanced across all classes |
| **Weighted F1-Score** | 0.3191 | Accounts for class distribution |
| **ROC AUC (OvR)** | 0.8059 | Strong discriminative capability |
| **Matthews Correlation** | 0.2602 | Better than random classification |
| **Cohen's Kappa** | 0.2575 | Moderate agreement |

### üìà Per-Class Performance

| Defect Class | F1-Score | Precision | Recall | Key Insights |
|--------------|----------|-----------|--------|--------------|
| **Near-full** | 0.618 | 0.464 | 0.923 | ‚úÖ Best performing class |
| **none** | 0.457 | 0.500 | 0.421 | ‚úÖ Improved from baseline |
| **Donut** | 0.358 | 0.407 | 0.319 | üîÑ Moderate performance |
| **Random** | 0.309 | 0.308 | 0.310 | üîÑ Consistent metrics |
| **Edge-Loc** | 0.291 | 0.273 | 0.313 | ‚ö†Ô∏è Confused with Edge-Ring |
| **Loc** | 0.263 | 0.284 | 0.244 | ‚ö†Ô∏è Needs improvement |
| **Center** | 0.241 | 0.234 | 0.249 | ‚ö†Ô∏è Low performance |
| **Edge-Ring** | 0.173 | 0.183 | 0.164 | ‚ùå Most challenging class |
| **Scratch** | 0.156 | 0.263 | 0.111 | ‚ùå Low recall |

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Wafer Maps    ‚îÇ    ‚îÇ  Sensor Data    ‚îÇ
‚îÇ   (25√ó27 px)    ‚îÇ    ‚îÇ  (590 ‚Üí 6 dims) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                      ‚îÇ
          ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CNN Branch    ‚îÇ    ‚îÇ   MLP Branch    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ Conv2D(32)‚Üí64‚Üí  ‚îÇ    ‚îÇ Dense(512)‚Üí     ‚îÇ
‚îÇ 128‚Üí256         ‚îÇ    ‚îÇ 256‚Üí128         ‚îÇ
‚îÇ + BatchNorm     ‚îÇ    ‚îÇ + Dropout       ‚îÇ
‚îÇ + MaxPool       ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ Attention Fusion ‚îÇ
           ‚îÇ   Œ±_img, Œ±_sensor ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ Classification  ‚îÇ
           ‚îÇ  Head (9 classes) ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üî¨ Technical Components

1. **CNN Branch**: Processes 25√ó27 wafer map images through 4 convolutional layers
2. **MLP Branch**: Handles physics-grouped sensor data (590 sensors ‚Üí 6 groups)
3. **Attention Fusion**: Dynamic weighting of image vs. sensor contributions
4. **Classification Head**: Final layers with dropout regularization

## üìÅ Repository Structure

```
wafer-defect-detection/
‚îú‚îÄ‚îÄ üìì notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ main_analysis.ipynb          # Primary analysis notebook
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.ipynb     # Data cleaning and preparation
‚îÇ   ‚îî‚îÄ‚îÄ model_evaluation.ipynb      # Detailed results analysis
‚îú‚îÄ‚îÄ üìÑ docs/
‚îÇ   ‚îú‚îÄ‚îÄ project_report.pdf          # Complete technical report
‚îÇ   ‚îî‚îÄ‚îÄ architecture_diagram.png    # Visual model architecture
‚îú‚îÄ‚îÄ üìä results/
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png        # Confusion matrix visualization
‚îÇ   ‚îú‚îÄ‚îÄ per_class_metrics.csv       # Detailed performance metrics
‚îÇ   ‚îî‚îÄ‚îÄ training_history.json       # Training progress logs
‚îú‚îÄ‚îÄ üîß src/
‚îÇ   ‚îú‚îÄ‚îÄ models.py                   # Model architecture definitions
‚îÇ   ‚îú‚îÄ‚îÄ data_utils.py               # Data loading and preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ training.py                 # Training loops and utilities
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py               # Evaluation metrics and plots
‚îú‚îÄ‚îÄ üìã requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ üèÉ‚Äç‚ôÇÔ∏è quick_start.py               # Simple demo script
‚îî‚îÄ‚îÄ üìñ README.md                    # This file
```

## üöÄ Quick Start

### Option 1: Google Colab (Recommended)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/your-notebook-id)

1. Click the "Open in Colab" button above
2. Upload the required datasets (instructions in notebook)
3. Run all cells to reproduce results
4. Explore the interactive visualizations

### Option 2: Local Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/wafer-defect-detection.git
cd wafer-defect-detection

# Install dependencies
pip install -r requirements.txt

# Run quick demo
python quick_start.py
```

## üìä Datasets

### WM-811K Wafer Map Dataset
- **Size**: 811,457 labeled wafer images
- **Classes**: 9 defect categories (Center, Donut, Edge-Loc, Edge-Ring, Loc, Near-full, Random, Scratch, none)
- **Format**: 25√ó27 pixel grayscale images
- **Source**: [Kaggle WM-811K](https://www.kaggle.com/datasets/qingyi/wm811k-wafer-map)

### SECOM Manufacturing Dataset
- **Size**: 1,567 samples with 590 sensor measurements
- **Features**: Process sensor readings during semiconductor fabrication
- **Labels**: Pass/Fail quality outcomes
- **Source**: [UCI SECOM Dataset](https://archive.ics.uci.edu/ml/datasets/secom)

## üõ†Ô∏è Technical Requirements

### Dependencies
```
torch>=1.9.0
torchvision>=0.10.0
scikit-learn>=1.0.0
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
Pillow>=8.3.0
tqdm>=4.62.0
```

### Hardware Requirements
- **Minimum**: 8GB RAM, CPU-only training possible
- **Recommended**: 16GB RAM + GPU (CUDA support)
- **Optimal**: Google Colab Pro with high-RAM runtime

## üß† Model Architecture Details

### Physics-Based Sensor Grouping
```python
sensor_groups = {
    'pressure': sensors[0:98],      # Pressure measurements
    'temperature': sensors[98:196], # Temperature readings  
    'flow': sensors[196:294],       # Flow rate data
    'chemistry': sensors[294:392],  # Chemical concentrations
    'power': sensors[392:490],      # Power/voltage levels
    'timing': sensors[490:590]      # Timing/cycle parameters
}
```

### Attention Mechanism
The model uses learnable attention weights to dynamically balance image and sensor contributions:

```
Œ±_img = W_att^T * tanh(W_img * f_img)
Œ±_sensor = W_att^T * tanh(W_sensor * f_sensor)
[Œ≤_img, Œ≤_sensor] = softmax([Œ±_img, Œ±_sensor])
f_fused = Œ≤_img * f_img + Œ≤_sensor * f_sensor
```

## üìà Training Strategy

- **Loss Function**: Weighted Cross-Entropy (addresses class imbalance)
- **Optimizer**: AdamW with weight decay (0.01)
- **Learning Rate**: 0.001 with ReduceLROnPlateau scheduling
- **Regularization**: Dropout (0.1-0.3), Batch Normalization
- **Early Stopping**: Patience of 10 epochs on validation accuracy
- **Data Augmentation**: Random rotation, flipping, translation for images

## üîç Key Findings

### ‚úÖ Strengths
1. **Multi-modal Approach**: Successfully integrates heterogeneous data sources
2. **Attention Mechanism**: Provides interpretable modality weighting
3. **Physics-Informed Design**: Leverages domain knowledge in sensor grouping
4. **Class Balance**: Reduces "none" class dominance from 97% to 11%

### ‚ö†Ô∏è Areas for Improvement
1. **Overall Accuracy**: 34% accuracy needs significant improvement
2. **Class Confusion**: High misclassification between similar defect types
3. **Edge Defects**: Poor performance on Edge-Ring and Edge-Loc classes
4. **Decision Boundaries**: Gap between AUC (0.81) and accuracy suggests suboptimal thresholds

### üéØ Future Work
- **Advanced Architectures**: Vision Transformers, ResNet backbones
- **Data Augmentation**: More sophisticated augmentation strategies
- **Ensemble Methods**: Combining multiple model predictions
- **Uncertainty Quantification**: Bayesian neural networks for confidence estimation

## üë• Team Contributors

| Name | Role | Contributions |
|------|------|---------------|
| **Anish Chhabra** | Project Lead | CNN architecture, methodology, literature review |
| **Parth Aditya** | Data Engineer | Dataset curation, preprocessing, evaluation framework |
| **Joey Madigan** | ML Architect | Multi-modal fusion, sensor processing, uncertainty quantification |

## ü§ñ AI Assistance Disclosure

**This project utilized AI assistance for code development and documentation:**

- **Google Colab with Gemini**: Used for generating code comments, landmarks for output visualization, and debugging assistance
- **Code Enhancement**: AI helped create clear section markers, detailed comments, and improved code readability
- **Documentation**: AI assisted in creating comprehensive docstrings and README structure
- **Debugging**: AI tools helped identify and resolve tensor shape mismatches and data loading issues

**Human Contributions**: All core algorithms, model architecture design, experimental methodology, and scientific insights were developed by the human team members. AI was used as a coding assistant and documentation tool.

## üìÑ Citation

If you use this work in your research, please cite:

```bibtex
@article{chhabra2025multimodal,
  title={Multi-Modal Deep Learning for Semiconductor Wafer Defect Detection Using Image and Sensor Fusion},
  author={Chhabra, Anish and Aditya, Parth and Madigan, Joey},
  journal={CS230 Deep Learning Project},
  year={2025},
  institution={University of Wisconsin-Madison}
}
```

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üîó Links

- **Project Report**: [PDF](docs/project_report.pdf)
- **Google Colab Notebook**: [Interactive Demo](https://colab.research.google.com/drive/your-notebook-id)
- **WM-811K Dataset**: [Kaggle](https://www.kaggle.com/datasets/qingyi/wm811k-wafer-map)
- **SECOM Dataset**: [UCI Repository](https://archive.ics.uci.edu/ml/datasets/secom)

## üìû Contact

For questions or collaboration opportunities:

- **Anish Chhabra**: chhabra8@wisc.edu
- **Parth Aditya**: paditya2@wisc.edu  
- **Joey Madigan**: jpmadigan@wisc.edu

---

‚≠ê **Star this repository if you found it helpful!** ‚≠ê
